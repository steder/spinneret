"""
This script takes a sitemap file generated by `spider.py` and
spins up a number of workers (threads/processes/greenlets) to
hit those URLs.

A few additional features would be helpful here like:
 - remembering stats about the URLs requested
   + min, max, and avg response times
   + min, max, and avg response sizes
 - failure rates for URLs
 - response code counts (e.g.: 95 2xx, 4 4xx, and 1 5xx)

This script runs forever (or until it's killed) checking
these URLs and periodically outputing stats.

"""
import sys
import itertools
import timeit

from gevent import pool
from gevent import monkey
import requests
import yaml



monkey.patch_socket()



def sitemap_to_urls(sitemap_path):
    with open(sitemap_path, "r") as sitemap_file:
        sitemap = yaml.load(sitemap_file.read())

    urls = []

    def append_paths(parent_path, d, urls):
        for key in d:
            print parent_path, key
            url_path = parent_path + (key,)
            url = "/".join(url_path)
            urls.append(url)
            append_paths(url_path, d[key], urls)
    append_paths((), sitemap, urls)

    return urls



def takeN(size, iterable):
    batch = []
    for n, i in enumerate(iterable):
        batch.append(i)
        if n % size == 0:
            yield batch
            batch = []
        yield batch


base = "http://beta.threadless.com"


def get_url(url):
    absolute_url = base + url
    print "getting: %s ... "%(absolute_url,)
    start = timeit.default_timer()
    try:
        resp = requests.get(absolute_url, timeout=30.0)
    except requests.exceptions.Timeout:
        stop = timeit.default_timer()
        elapsed = stop - start
        print "%s: Timed out (%s)"%(absolute_url, elapsed)
    except requests.exceptions.SSLError:
        stop = timeit.default_timer()
        elapsed = stop - start
        print "%s: SSL Error (%s)"%(absolute_url, elapsed)
    except requests.exceptions.ConnectionError:
        stop = timeit.default_timer()
        elapsed = stop - start
        print "%s: Connection Error (%s)"%(absolute_url, elapsed)
    else:
        stop = timeit.default_timer()
        elapsed = stop - start
        print "%s: %s (%s) (%4f)"%(absolute_url, resp.status_code, resp.reason, elapsed)



def main(base_url, sitemap_path):
    global base
    base = base_url

    inflight = 10

    urls = sitemap_to_urls(sitemap_path)

    p = pool.Pool(inflight)

    while True:
        p.map(get_url, urls)
        p.join()

