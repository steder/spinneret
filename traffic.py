"""
This script takes a sitemap file generated by `spider.py` and
spins up a number of workers (threads/processes/greenlets) to
hit those URLs.

A few additional features would be helpful here like:
 - remembering stats about the URLs requested
   + min, max, and avg response times
   + min, max, and avg response sizes
 - failure rates for URLs
 - response code counts (e.g.: 95 2xx, 4 4xx, and 1 5xx)

This script runs forever (or until it's killed) checking
these URLs and periodically outputing stats.

"""
import os
import timeit

import gevent
from gevent import monkey
import requests
import yaml



monkey.patch_socket()



def sitemap_to_urls(sitemap_path):
    with open(sitemap_path, "r") as sitemap_file:
        sitemap = yaml.load(sitemap_file.read())

    urls = []

    def append_paths(parent_path, d, urls):
        for key in d:
            print parent_path, key
            url_path = parent_path + (key,)
            url = "/".join(url_path)
            urls.append(url)
            append_paths(url_path, d[key], urls)
    append_paths((), sitemap, urls)
    print urls
    return urls


def main(sitemap_path):
    urls = sitemap_to_urls(sitemap_path)

    def get_url(base, url):
        absolute_url = base + url
        print "getting: %s ... "%(absolute_url,)
        start = timeit.default_timer()
        try:
            resp = requests.get(absolute_url, timeout=30.0)
        except requests.exceptions.Timeout:
            stop = timeit.default_timer()
            elapsed = stop - start
            print "%s: Timed out (%s)"%(absolute_url, elapsed)
        else:
            stop = timeit.default_timer()
            elapsed = stop - start
            print "%s: %s (%s) (%4f)"%(absolute_url, resp.status_code, resp.reason, elapsed)


    jobs = []
    for url in urls:
        job = gevent.spawn(get_url, "http://beta.threadless.com", url)
        jobs.append(job)

    gevent.joinall(jobs)

if __name__=="__main__":
    main("sitemap.yaml")

